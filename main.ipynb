{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86ddc4a1",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df963393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tempfile\n",
    "from moviepy import VideoFileClip\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import librosa\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb792a27",
   "metadata": {},
   "source": [
    "### Function: `block_difference`\n",
    "- Compares two frames by dividing them into blocks\n",
    "- Calculates the mean absolute difference between corresponding blocks \n",
    "- Returns the average block difference across the whole frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a03d719d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_difference(frame1, frame2, block_size=16):\n",
    "    height, width = frame1.shape\n",
    "    total_diff = 0\n",
    "    num_blocks = 0\n",
    "\n",
    "    for y in range(0, height, block_size):\n",
    "        for x in range(0, width, block_size):\n",
    "            block1 = frame1[y:y+block_size, x:x+block_size]\n",
    "            block2 = frame2[y:y+block_size, x:x+block_size]\n",
    "            if block1.shape == block2.shape:\n",
    "                diff = np.abs(block1.astype(int) - block2.astype(int)).mean()\n",
    "                total_diff += diff\n",
    "                num_blocks += 1\n",
    "\n",
    "    return total_diff / num_blocks if num_blocks > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27bedcd",
   "metadata": {},
   "source": [
    "### Function: `extract_keyframes_from_shot`\n",
    "- Extracts keyframes from a list of frames using block difference\n",
    "- Uses the first frame as reference, then adds new frames when difference exceeds a threshold\n",
    "- Resizes keyframes to 224x224\n",
    "- Ensures exactly 15 keyframes by padding with the last frame if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24e9dc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keyframes_from_shot(frames, fps, block_threshold=20):\n",
    "    duration = len(frames) / fps\n",
    "    target_num_keyframes = 15\n",
    "    keyframes = []\n",
    "\n",
    "    if len(frames) == 0:\n",
    "        return []\n",
    "\n",
    "    ref_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)\n",
    "    resized_frame = cv2.resize(frames[0], (224, 224))\n",
    "    keyframes.append(resized_frame)\n",
    "\n",
    "    for i, frame in enumerate(frames[1:], start=1):\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        diff = block_difference(ref_gray, gray)\n",
    "        if diff > block_threshold:\n",
    "            resized = cv2.resize(frame, (224, 224))\n",
    "            keyframes.append(resized)\n",
    "\n",
    "        if len(keyframes) >= target_num_keyframes:\n",
    "            break\n",
    "\n",
    "    while len(keyframes) < target_num_keyframes:\n",
    "        keyframes.append(keyframes[-1].copy())\n",
    "\n",
    "    return keyframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e08affc",
   "metadata": {},
   "source": [
    "### Function: `extract_audio_shot`\n",
    "- Extracts a segment of audio from a given audio file\n",
    "- Takes `start_time` and `end_time` in seconds\n",
    "- Saves the extracted audio segment as a `.wav` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56fea99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_shot(audio_path, start_time, end_time, save_path):\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "    start_ms = int(start_time * 1000)\n",
    "    end_ms = int(end_time * 1000)\n",
    "    segment = audio[start_ms:end_ms]\n",
    "    segment.export(save_path, format=\"wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a90adc9",
   "metadata": {},
   "source": [
    "### Function: `extract_audio_temp`\n",
    "- Extracts the entire audio track from a video\n",
    "- Saves it temporarily as a `.wav` file\n",
    "- Returns the path of the temporary audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62baec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_temp(video_path):\n",
    "    video = VideoFileClip(video_path)\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as temp_audio_file:\n",
    "        temp_audio_path = temp_audio_file.name\n",
    "    video.audio.write_audiofile(temp_audio_path)\n",
    "    return temp_audio_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5db51a5",
   "metadata": {},
   "source": [
    "### Function: `extract_shots_and_keyframes_with_audio`\n",
    "- Splits a video into shots based on histogram difference\n",
    "- Extracts **keyframes** from each shot using block difference\n",
    "- Saves the **corresponding audio segment** of each shot as `.wav`\n",
    "- Returns a list of samples, where each sample contains:  \n",
    "  - keyframes  \n",
    "  - audio file path  \n",
    "  - start time and end time of the shot  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11562f83",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0a6e797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_shots_and_keyframes_with_audio(video_path, hist_threshold=30, block_threshold=20, fps_cap=30, save_audio_dir=\"audio_shots\"):\n",
    "    os.makedirs(save_audio_dir, exist_ok=True)\n",
    "\n",
    "    audio_path = extract_audio_temp(video_path)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    fps = min(fps, fps_cap)\n",
    "\n",
    "    prev_hist = None\n",
    "    frames = []\n",
    "    all_samples = []\n",
    "    shot_start_frame = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n",
    "        hist = cv2.normalize(hist, hist).flatten()\n",
    "\n",
    "        if prev_hist is not None:\n",
    "            diff = cv2.compareHist(prev_hist, hist, cv2.HISTCMP_BHATTACHARYYA)\n",
    "            if diff > hist_threshold / 100:\n",
    "                shot_end_frame = shot_start_frame + len(frames)\n",
    "                start_time = shot_start_frame / fps\n",
    "                end_time = shot_end_frame / fps\n",
    "\n",
    "                keyframes = extract_keyframes_from_shot(frames, fps, block_threshold)\n",
    "\n",
    "                if keyframes:\n",
    "                    audio_filename = f\"{os.path.basename(video_path)}_{int(start_time*1000)}_{int(end_time*1000)}.wav\"\n",
    "                    audio_save_path = os.path.join(save_audio_dir, audio_filename)\n",
    "                    extract_audio_shot(audio_path, start_time, end_time, audio_save_path)\n",
    "\n",
    "                    all_samples.append({\n",
    "                        \"keyframes\": keyframes,\n",
    "                        \"audio_path\": audio_save_path,\n",
    "                        \"start_time\": start_time,\n",
    "                        \"end_time\": end_time\n",
    "                    })\n",
    "\n",
    "                shot_start_frame = shot_end_frame\n",
    "                frames = []\n",
    "\n",
    "        frames.append(frame.copy())\n",
    "        prev_hist = hist\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    os.remove(audio_path)\n",
    "\n",
    "    return all_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2c7fe9",
   "metadata": {},
   "source": [
    "### Function: `process_category_folder`\n",
    "Processes all video files in a category folder, extracts shots with keyframes and audio, and assigns the given label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c062407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_category_folder(category_path, audio_folder_path, label):\n",
    "    samples = []\n",
    "\n",
    "    for filename in os.listdir(category_path):\n",
    "        if filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "            video_path = os.path.join(category_path, filename)\n",
    "            audio_path = os.path.join(audio_folder_path, os.path.splitext(filename)[0] + \".wav\")\n",
    "            print(f\"Processing: {video_path}\")\n",
    "            shot_samples = extract_shots_and_keyframes_with_audio(video_path)\n",
    "            for sample in shot_samples:\n",
    "                sample[\"label\"] = label\n",
    "                samples.append(sample)\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95665406",
   "metadata": {},
   "source": [
    "### Main Script\n",
    "Iterates over all categories, processes their videos, and collects all samples into a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8973fa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"C:\\Users\\LEGION\\Desktop\\Violence Detection-PyTorch\"\n",
    "categories = [\"bloody\", \"explosions\", \"fight\", \"non-violence\"]\n",
    "\n",
    "all_samples = []\n",
    "\n",
    "for category in categories:\n",
    "    video_folder = os.path.join(base_path, category)\n",
    "    audio_folder = os.path.join(base_path, category)\n",
    "    category_samples = process_category_folder(video_folder, audio_folder, category)\n",
    "    all_samples.extend(category_samples)\n",
    "\n",
    "print(\"Total samples:\", len(all_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b51b78",
   "metadata": {},
   "source": [
    "### Data Preparation (Images + Audio + Labels)\n",
    "\n",
    "- Initializes empty lists for **image sequences**, **audio spectrograms**, and **labels**\n",
    "- Ensures each sample has exactly **15 frames** (by truncating or padding with the last frame)\n",
    "- Loads the corresponding **audio clip**, converts it into a **mel-spectrogram (128 mel bins)**, and pads/truncates it to a fixed length of **200 time steps**\n",
    "- Appends processed image frames, spectrogram, and label to their lists\n",
    "- Encodes labels into numeric form using `LabelEncoder`\n",
    "- Converts all lists into NumPy arrays and prints their shapes for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e04e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_images = []\n",
    "X_audio = []\n",
    "y_labels = []\n",
    "\n",
    "time_steps = 15\n",
    "mel_bins = 128\n",
    "max_audio_len = 200\n",
    "\n",
    "for sample in all_samples:\n",
    "    frames = sample[\"keyframes\"]\n",
    "    if len(frames) < time_steps:\n",
    "        frames += [frames[-1]] * (time_steps - len(frames))\n",
    "    else:\n",
    "        frames = frames[:time_steps]\n",
    "\n",
    "    y_audio, sr = librosa.load(sample[\"audio_path\"], sr=22050)\n",
    "    mel = librosa.feature.melspectrogram(y=y_audio, sr=sr, n_mels=mel_bins)\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "\n",
    "    if mel_db.shape[1] < max_audio_len:\n",
    "        pad_width = max_audio_len - mel_db.shape[1]\n",
    "        mel_db = np.pad(mel_db, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        mel_db = mel_db[:, :max_audio_len]\n",
    "\n",
    "    X_images.append(np.array(frames))\n",
    "    X_audio.append(mel_db)\n",
    "    y_labels.append(sample[\"label\"])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y_labels)\n",
    "\n",
    "X_images = np.array(X_images)\n",
    "X_audio = np.array(X_audio)\n",
    "y_encoded = np.array(y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fd8bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# joblib.dump(encoder, \"label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a262e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_img_train, X_img_test, X_audio_train, X_audio_test, y_train, y_test = train_test_split(\n",
    "    X_images, X_audio, y_encoded,\n",
    "    test_size=0.15,\n",
    "    stratify=y_encoded,\n",
    "    random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "905ff313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"train_data_with_audio_short.pkl\", \"wb\") as f:\n",
    "#     pickle.dump((X_img_train, X_audio_train, y_train), f)\n",
    "\n",
    "# with open(\"test_data_with_audio_short.pkl\", \"wb\") as f:\n",
    "#     pickle.dump((X_img_test, X_audio_test, y_test), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf1c384",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_data_with_audio_short.pkl\", \"rb\") as f:\n",
    "    X_img_train, X_audio_train, y_train = pickle.load(f)\n",
    "\n",
    "with open(\"test_data_with_audio_short.pkl\", \"rb\") as f:\n",
    "    X_img_test, X_audio_test, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426f4f13",
   "metadata": {},
   "source": [
    "### `CartoonViolenceMultiModalDataset` Class\n",
    "\n",
    "- Custom PyTorch `Dataset` for multimodal data (video frames + audio spectrograms + labels)\n",
    "- Supports **data augmentation** for image frames (random flips & rotations) when `augment=True`\n",
    "- `__len__`: returns the total number of samples\n",
    "- `__getitem__`:  \n",
    "  - Retrieves one sequence of frames, its spectrogram, and the label\n",
    "  - Transforms each frame into a tensor (with or without augmentation)\n",
    "  - Stacks all frames into a single tensor with shape `[T, C, H, W]` (time, channels, height, width)\n",
    "  - Converts spectrogram into a tensor of shape `[1, n_mels, time]`\n",
    "  - Returns `(frames_tensor, audio_tensor, label)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caed5fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartoonViolenceMultiModalDataset(Dataset):\n",
    "    def __init__(self, img_sequences, audio_spectrograms, labels, augment=False):\n",
    "        self.img_sequences = img_sequences\n",
    "        self.audio_spectrograms = audio_spectrograms\n",
    "        self.labels = labels\n",
    "        self.augment = augment\n",
    "\n",
    "        self.transform = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomRotation(15),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "        self.basic_transform = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_seq = self.img_sequences[idx]\n",
    "        audio_spec = self.audio_spectrograms[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        frames = []\n",
    "        for frame in img_seq:\n",
    "            frame_tensor = self.transform(frame) if self.augment else self.basic_transform(frame)\n",
    "            frames.append(frame_tensor)\n",
    "\n",
    "        frames_tensor = torch.stack(frames)\n",
    "        audio_tensor = torch.tensor(audio_spec, dtype=torch.float32)\n",
    "\n",
    "        return frames_tensor, audio_tensor.unsqueeze(0), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1984f148",
   "metadata": {},
   "source": [
    "### Class: `CNNLSTM_Audio`\n",
    "\n",
    "- **Purpose**: A multimodal neural network that combines **image sequences** and **audio spectrograms** for classification.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 🔹 Image Pathway\n",
    "- Uses a series of **CNN layers** to extract spatial features from each frame.  \n",
    "- Flattens the output and feeds it into an **LSTM** to capture temporal dependencies across frames.  \n",
    "- Produces a final image representation vector (size 128).  \n",
    "\n",
    "---\n",
    "\n",
    "#### 🔹 Audio Pathway\n",
    "- Applies **1D CNN layers** to process the mel-spectrogram input (treating it like a sequence).  \n",
    "- Passes the output through an **LSTM** to capture temporal audio features.  \n",
    "- Produces a final audio representation vector (size 64).  \n",
    "\n",
    "---\n",
    "\n",
    "#### 🔹 Fusion and Classification\n",
    "- Concatenates image and audio representations into a single feature vector `[128 + 64]`.  \n",
    "- Applies a **dropout layer** and a **fully connected layer** to classify into `num_classes`.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 🔹 Forward Pass\n",
    "1. Processes image sequence through CNN → Flatten → LSTM → final frame output.  \n",
    "2. Processes audio spectrogram through CNN → reshape → LSTM → final timestep output.  \n",
    "3. Concatenates image and audio outputs.  \n",
    "4. Passes fused vector through the classifier to get predictions.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b04ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTM_Audio(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNLSTM_Audio, self).__init__()\n",
    "\n",
    "        # Image pathway\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.lstm = nn.LSTM(128 * 28 * 28, 128, batch_first=True)\n",
    "\n",
    "        # Audio pathway\n",
    "        self.audio_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(64, 32, kernel_size=3, padding=1), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.audio_lstm = nn.LSTM(input_size=32, hidden_size=64, batch_first=True)\n",
    " \n",
    "        # Fusion and classifier\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128 + 64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_seq, audio_spec):\n",
    "        B, T, C, H, W = img_seq.shape\n",
    "\n",
    "        # Image branch\n",
    "        x = img_seq.view(-1, C, H, W)\n",
    "        x = self.cnn(x)\n",
    "        x = self.flatten(x)\n",
    "        x = x.view(B, T, -1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        # Audio branch\n",
    "        a = audio_spec.squeeze(1)  # [B, 128, time]\n",
    "        a = self.audio_conv(a)     # [B, 32, time']\n",
    "        a = a.permute(0, 2, 1)     # [B, time', 32]\n",
    "        a, _ = self.audio_lstm(a)\n",
    "        a = a[:, -1, :]\n",
    "\n",
    "        # Fusion\n",
    "        fused = torch.cat((x, a), dim=1)  # [B, 128 + 64]\n",
    "\n",
    "        # Classification\n",
    "        out = self.fc(fused)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b63b3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04ed6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CartoonViolenceMultiModalDataset(X_img_train, X_audio_train, y_train_tensor, augment=True)\n",
    "test_dataset = CartoonViolenceMultiModalDataset(X_img_test, X_audio_test, y_test_tensor, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba2216",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87f1cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNLSTM_Audio(num_classes=len(np.unique(y_train))).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23abbd40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
