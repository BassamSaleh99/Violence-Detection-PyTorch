{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c34bf87d-367a-43bc-bbe2-e652f512713d",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed60ca09-4bb4-4299-8731-c2307cd76ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tempfile\n",
    "from moviepy import VideoFileClip\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import librosa\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14103b9b-62c0-47f2-bfc6-0fc843c34410",
   "metadata": {},
   "source": [
    "### Function: `block_difference`\n",
    "- Compares two frames by dividing them into blocks\n",
    "- Calculates the mean absolute difference between corresponding blocks \n",
    "- Returns the average block difference across the whole frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91e60648-cce7-4612-94f6-61ededa3d1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_difference(frame1, frame2, block_size=16):\n",
    "    height, width = frame1.shape\n",
    "    total_diff = 0\n",
    "    num_blocks = 0\n",
    "\n",
    "    for y in range(0, height, block_size):\n",
    "        for x in range(0, width, block_size):\n",
    "            block1 = frame1[y:y+block_size, x:x+block_size]\n",
    "            block2 = frame2[y:y+block_size, x:x+block_size]\n",
    "            if block1.shape == block2.shape:\n",
    "                diff = np.abs(block1.astype(int) - block2.astype(int)).mean()\n",
    "                total_diff += diff\n",
    "                num_blocks += 1\n",
    "\n",
    "    return total_diff / num_blocks if num_blocks > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debf8ad3-0ff4-41a2-90a9-2f7b0e2e7f0a",
   "metadata": {},
   "source": [
    "### Function: `extract_keyframes_from_shot`\n",
    "- Extracts keyframes from a list of frames using block difference\n",
    "- Uses the first frame as reference, then adds new frames when difference exceeds a threshold\n",
    "- Resizes keyframes to 224x224\n",
    "- Ensures exactly 15 keyframes by padding with the last frame if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c8b9b1d-8d33-40da-a8c7-fa29c2f2197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keyframes_from_shot(frames, fps, block_threshold=20):\n",
    "    duration = len(frames) / fps\n",
    "    target_num_keyframes = 15\n",
    "    keyframes = []\n",
    "\n",
    "    if len(frames) == 0:\n",
    "        return []\n",
    "\n",
    "    ref_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)\n",
    "    resized_frame = cv2.resize(frames[0], (224, 224))\n",
    "    keyframes.append(resized_frame)\n",
    "\n",
    "    for i, frame in enumerate(frames[1:], start=1):\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        diff = block_difference(ref_gray, gray)\n",
    "        if diff > block_threshold:\n",
    "            resized = cv2.resize(frame, (224, 224))\n",
    "            keyframes.append(resized)\n",
    "\n",
    "        if len(keyframes) >= target_num_keyframes:\n",
    "            break\n",
    "\n",
    "    while len(keyframes) < target_num_keyframes:\n",
    "        keyframes.append(keyframes[-1].copy())\n",
    "\n",
    "    return keyframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c383752-5284-4745-9bec-0aeff31ddd0f",
   "metadata": {},
   "source": [
    "### Function: `extract_audio_shot`\n",
    "- Extracts a segment of audio from a given audio file\n",
    "- Takes `start_time` and `end_time` in seconds\n",
    "- Saves the extracted audio segment as a `.wav` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "716fa104-39c9-4b0d-9621-d15ac5515741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_shot(audio_path, start_time, end_time, save_path):\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "    start_ms = int(start_time * 1000)\n",
    "    end_ms = int(end_time * 1000)\n",
    "    segment = audio[start_ms:end_ms]\n",
    "    segment.export(save_path, format=\"wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8c047d-179f-4f39-8243-658c16c19cea",
   "metadata": {},
   "source": [
    "### Function: `extract_audio_temp`\n",
    "- Extracts the entire audio track from a video\n",
    "- Saves it temporarily as a `.wav` file\n",
    "- Returns the path of the temporary audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29a8fa3d-ec30-4bbc-b1ee-68ee3e3d9396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_temp(video_path):\n",
    "    video = VideoFileClip(video_path)\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as temp_audio_file:\n",
    "        temp_audio_path = temp_audio_file.name\n",
    "    video.audio.write_audiofile(temp_audio_path)\n",
    "    return temp_audio_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e24717-5574-46a6-86be-2fa9d5ee129d",
   "metadata": {},
   "source": [
    "### Function: `extract_shots_and_keyframes_with_audio`\n",
    "- Splits a video into shots based on histogram difference\n",
    "- Extracts **keyframes** from each shot using block difference\n",
    "- Saves the **corresponding audio segment** of each shot as `.wav`\n",
    "- Returns a list of samples, where each sample contains:  \n",
    "  - keyframes  \n",
    "  - audio file path  \n",
    "  - start time and end time of the shot  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "637628bd-94b6-4acf-86f4-10a55275a80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_shots_and_keyframes_with_audio(video_path, hist_threshold=30, block_threshold=20, fps_cap=30, save_audio_dir=\"audio_shots\"):\n",
    "    os.makedirs(save_audio_dir, exist_ok=True)\n",
    "\n",
    "    audio_path = extract_audio_temp(video_path)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    fps = min(fps, fps_cap)\n",
    "\n",
    "    prev_hist = None\n",
    "    frames = []\n",
    "    all_samples = []\n",
    "    shot_start_frame = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n",
    "        hist = cv2.normalize(hist, hist).flatten()\n",
    "\n",
    "        if prev_hist is not None:\n",
    "            diff = cv2.compareHist(prev_hist, hist, cv2.HISTCMP_BHATTACHARYYA)\n",
    "            if diff > hist_threshold / 100:\n",
    "                shot_end_frame = shot_start_frame + len(frames)\n",
    "                start_time = shot_start_frame / fps\n",
    "                end_time = shot_end_frame / fps\n",
    "\n",
    "                keyframes = extract_keyframes_from_shot(frames, fps, block_threshold)\n",
    "\n",
    "                if keyframes:\n",
    "                    audio_filename = f\"{os.path.basename(video_path)}_{int(start_time*1000)}_{int(end_time*1000)}.wav\"\n",
    "                    audio_save_path = os.path.join(save_audio_dir, audio_filename)\n",
    "                    extract_audio_shot(audio_path, start_time, end_time, audio_save_path)\n",
    "\n",
    "                    all_samples.append({\n",
    "                        \"keyframes\": keyframes,\n",
    "                        \"audio_path\": audio_save_path,\n",
    "                        \"start_time\": start_time,\n",
    "                        \"end_time\": end_time\n",
    "                    })\n",
    "\n",
    "                shot_start_frame = shot_end_frame\n",
    "                frames = []\n",
    "\n",
    "        frames.append(frame.copy())\n",
    "        prev_hist = hist\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    os.remove(audio_path)\n",
    "\n",
    "    return all_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3b08aa-5744-46e5-be62-e450ebb4c014",
   "metadata": {},
   "source": [
    "### Function: `process_category_folder`\n",
    "Processes all video files in a category folder, extracts shots with keyframes and audio, and assigns the given label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9d0e045-2c6f-4364-ab89-f3fd7e078630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_category_folder(category_path, audio_folder_path, label):\n",
    "    samples = []\n",
    "\n",
    "    for filename in os.listdir(category_path):\n",
    "        if filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "            video_path = os.path.join(category_path, filename)\n",
    "            audio_path = os.path.join(audio_folder_path, os.path.splitext(filename)[0] + \".wav\")\n",
    "            print(f\"Processing: {video_path}\")\n",
    "            shot_samples = extract_shots_and_keyframes_with_audio(video_path)\n",
    "            for sample in shot_samples:\n",
    "                sample[\"label\"] = label\n",
    "                samples.append(sample)\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7c9dbc-27d8-470e-b449-2b3b545cb94b",
   "metadata": {},
   "source": [
    "### Main Script\n",
    "Iterates over all categories, processes their videos, and collects all samples into a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4dc122-f02e-4f4a-8bd9-78412924d93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"C:\\Users\\LEGION\\Desktop\\Violence Detection-PyTorch\"\n",
    "categories = [\"bloody\", \"explosions\", \"fight\", \"non-violence\"]\n",
    "\n",
    "all_samples = []\n",
    "\n",
    "for category in categories:\n",
    "    video_folder = os.path.join(base_path, category)\n",
    "    audio_folder = os.path.join(base_path, category)\n",
    "    category_samples = process_category_folder(video_folder, audio_folder, category)\n",
    "    all_samples.extend(category_samples)\n",
    "\n",
    "print(\"Total samples:\", len(all_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e36f1a5-2903-490c-b679-99012bf2c6de",
   "metadata": {},
   "source": [
    "### Data Preparation (Images + Audio + Labels)\n",
    "\n",
    "- Initializes empty lists for **image sequences**, **audio spectrograms**, and **labels**\n",
    "- Ensures each sample has exactly **15 frames** (by truncating or padding with the last frame)\n",
    "- Loads the corresponding **audio clip**, converts it into a **mel-spectrogram (128 mel bins)**, and pads/truncates it to a fixed length of **200 time steps**\n",
    "- Appends processed image frames, spectrogram, and label to their lists\n",
    "- Encodes labels into numeric form using `LabelEncoder`\n",
    "- Converts all lists into NumPy arrays and prints their shapes for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec91479c-8f52-4076-91df-4fefb0109e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_images = []\n",
    "X_audio = []\n",
    "y_labels = []\n",
    "\n",
    "time_steps = 15\n",
    "mel_bins = 128\n",
    "max_audio_len = 200\n",
    "\n",
    "for sample in all_samples:\n",
    "    frames = sample[\"keyframes\"]\n",
    "    if len(frames) < time_steps:\n",
    "        frames += [frames[-1]] * (time_steps - len(frames))\n",
    "    else:\n",
    "        frames = frames[:time_steps]\n",
    "\n",
    "    y_audio, sr = librosa.load(sample[\"audio_path\"], sr=22050)\n",
    "    mel = librosa.feature.melspectrogram(y=y_audio, sr=sr, n_mels=mel_bins)\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "\n",
    "    if mel_db.shape[1] < max_audio_len:\n",
    "        pad_width = max_audio_len - mel_db.shape[1]\n",
    "        mel_db = np.pad(mel_db, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        mel_db = mel_db[:, :max_audio_len]\n",
    "\n",
    "    X_images.append(np.array(frames))\n",
    "    X_audio.append(mel_db)\n",
    "    y_labels.append(sample[\"label\"])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y_labels)\n",
    "\n",
    "X_images = np.array(X_images)\n",
    "X_audio = np.array(X_audio)\n",
    "y_encoded = np.array(y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb0663c2-7f44-4051-a08b-07089db9b548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_encoder.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import joblib\n",
    "# joblib.dump(encoder, \"label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d02e56df-cd93-4e08-a6b3-7c2eec54b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_img_train, X_img_test, X_audio_train, X_audio_test, y_train, y_test = train_test_split(\n",
    "    X_images, X_audio, y_encoded,\n",
    "    test_size=0.15,\n",
    "    stratify=y_encoded,\n",
    "    random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a5a1a9f-94ee-4543-82dc-e23c1725ffce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"train_data_with_audio_short.pkl\", \"wb\") as f:\n",
    "#     pickle.dump((X_img_train, X_audio_train, y_train), f)\n",
    "\n",
    "# with open(\"test_data_with_audio_short.pkl\", \"wb\") as f:\n",
    "#     pickle.dump((X_img_test, X_audio_test, y_test), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2b81c82-8058-4604-adc1-fc8396f1b08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_data_with_audio_short.pkl\", \"rb\") as f:\n",
    "    X_img_train, X_audio_train, y_train = pickle.load(f)\n",
    "\n",
    "with open(\"test_data_with_audio_short.pkl\", \"rb\") as f:\n",
    "    X_img_test, X_audio_test, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cdb69b-bfec-4a55-bae3-40c110b93fdf",
   "metadata": {},
   "source": [
    "### `CartoonViolenceMultiModalDataset` Class\n",
    "\n",
    "- Custom PyTorch `Dataset` for multimodal data (video frames + audio spectrograms + labels)\n",
    "- Supports **data augmentation** for image frames (random flips & rotations) when `augment=True`\n",
    "- `__len__`: returns the total number of samples\n",
    "- `__getitem__`:  \n",
    "  - Retrieves one sequence of frames, its spectrogram, and the label\n",
    "  - Transforms each frame into a tensor (with or without augmentation)\n",
    "  - Stacks all frames into a single tensor with shape `[T, C, H, W]` (time, channels, height, width)\n",
    "  - Converts spectrogram into a tensor of shape `[1, n_mels, time]`\n",
    "  - Returns `(frames_tensor, audio_tensor, label)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f495206-195a-4d53-becb-a8147a2c18ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartoonViolenceMultiModalDataset(Dataset):\n",
    "    def __init__(self, img_sequences, audio_spectrograms, labels, augment=False):\n",
    "        self.img_sequences = img_sequences\n",
    "        self.audio_spectrograms = audio_spectrograms\n",
    "        self.labels = labels\n",
    "        self.augment = augment\n",
    "\n",
    "        self.transform = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomRotation(15),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "        self.basic_transform = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_seq = self.img_sequences[idx]\n",
    "        audio_spec = self.audio_spectrograms[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        frames = []\n",
    "        for frame in img_seq:\n",
    "            frame_tensor = self.transform(frame) if self.augment else self.basic_transform(frame)\n",
    "            frames.append(frame_tensor)\n",
    "\n",
    "        frames_tensor = torch.stack(frames)\n",
    "        audio_tensor = torch.tensor(audio_spec, dtype=torch.float32)\n",
    "\n",
    "        return frames_tensor, audio_tensor.unsqueeze(0), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5048790-229b-4d7f-92fe-e3f54decf214",
   "metadata": {},
   "source": [
    "### Class: `CNNLSTM_Audio`\n",
    "\n",
    "- **Purpose**: A multimodal neural network that combines **image sequences** and **audio spectrograms** for classification.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 🔹 Image Pathway\n",
    "- Uses a series of **CNN layers** to extract spatial features from each frame.  \n",
    "- Flattens the output and feeds it into an **LSTM** to capture temporal dependencies across frames.  \n",
    "- Produces a final image representation vector (size 128).  \n",
    "\n",
    "---\n",
    "\n",
    "#### 🔹 Audio Pathway\n",
    "- Applies **1D CNN layers** to process the mel-spectrogram input (treating it like a sequence).  \n",
    "- Passes the output through an **LSTM** to capture temporal audio features.  \n",
    "- Produces a final audio representation vector (size 64).  \n",
    "\n",
    "---\n",
    "\n",
    "#### 🔹 Fusion and Classification\n",
    "- Concatenates image and audio representations into a single feature vector `[128 + 64]`.  \n",
    "- Applies a **dropout layer** and a **fully connected layer** to classify into `num_classes`.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 🔹 Forward Pass\n",
    "1. Processes image sequence through CNN → Flatten → LSTM → final frame output.  \n",
    "2. Processes audio spectrogram through CNN → reshape → LSTM → final timestep output.  \n",
    "3. Concatenates image and audio outputs.  \n",
    "4. Passes fused vector through the classifier to get predictions.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fc786db-9cb4-41c2-8d23-5f640c96c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTM_Audio(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNLSTM_Audio, self).__init__()\n",
    "\n",
    "        # Image pathway\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.lstm = nn.LSTM(128 * 28 * 28, 128, batch_first=True)\n",
    "\n",
    "        # Audio pathway\n",
    "        self.audio_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(64, 32, kernel_size=3, padding=1), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.audio_lstm = nn.LSTM(input_size=32, hidden_size=64, batch_first=True)\n",
    " \n",
    "        # Fusion and classifier\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128 + 64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_seq, audio_spec):\n",
    "        B, T, C, H, W = img_seq.shape\n",
    "\n",
    "        # Image branch\n",
    "        x = img_seq.view(-1, C, H, W)\n",
    "        x = self.cnn(x)\n",
    "        x = self.flatten(x)\n",
    "        x = x.view(B, T, -1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        # Audio branch\n",
    "        a = audio_spec.squeeze(1)  # [B, 128, time]\n",
    "        a = self.audio_conv(a)     # [B, 32, time']\n",
    "        a = a.permute(0, 2, 1)     # [B, time', 32]\n",
    "        a, _ = self.audio_lstm(a)\n",
    "        a = a[:, -1, :]\n",
    "\n",
    "        # Fusion\n",
    "        fused = torch.cat((x, a), dim=1)  # [B, 128 + 64]\n",
    "\n",
    "        # Classification\n",
    "        out = self.fc(fused)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b321ec56-fd40-4446-bd1d-9f5b935c2636",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5896538-0031-4790-a83b-bc147e1651cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CartoonViolenceMultiModalDataset(X_img_train, X_audio_train, y_train_tensor, augment=True)\n",
    "test_dataset = CartoonViolenceMultiModalDataset(X_img_test, X_audio_test, y_test_tensor, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b09dd6d4-9090-4018-bf0e-0aea4cc01ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3163a508-f781-40ac-a4bb-648730214ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNLSTM_Audio(num_classes=len(np.unique(y_train))).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdff1f4-6aa8-463d-a484-ed17bbd0a8c4",
   "metadata": {},
   "source": [
    "### Defines the loss function (CrossEntropy), optimizer (Adam), and gradient scaler for mixed-precision training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90cc0af6-7fbf-435c-b230-a029acfdd09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_11608\\1423562292.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05db098f-b35a-4b80-846c-f86df9089101",
   "metadata": {},
   "source": [
    "### Gradient Accumulation Training Loop\n",
    "\n",
    "This training loop uses **gradient accumulation** to effectively simulate a larger batch size by accumulating gradients over several mini-batches before performing an optimizer step. Mixed precision training with `torch.cuda.amp` is also used for efficiency\n",
    "\n",
    "#### Key Hyperparameters\n",
    "- `accumulation_steps = 8`: Number of steps to accumulate gradients before updating weights\n",
    "- `num_epochs = 30`: Total number of training epochs\n",
    "\n",
    "#### Workflow\n",
    "1. **Initialization**\n",
    "   - Track training losses and accuracies in lists\n",
    "   - Set the model to training mode with `model.train()`\n",
    "   - Initialize optimizer gradients with `optimizer.zero_grad()`\n",
    "\n",
    "2. **Mini-batch Training**\n",
    "   - For each batch `(img_seq, audio_spec, labels)`:\n",
    "     - Move inputs and labels to the device (GPU/CPU)\n",
    "     - Perform forward pass under `autocast()` for mixed precision\n",
    "     - Compute loss, normalized by `accumulation_steps`\n",
    "     - Backpropagate using `scaler.scale(loss).backward()`\n",
    "\n",
    "3. **Gradient Accumulation**\n",
    "   - After `accumulation_steps` iterations (or at the last batch):\n",
    "     - Update model weights with `scaler.step(optimizer)`\n",
    "     - Update the scaler with `scaler.update()`.\n",
    "     - Reset gradients with `optimizer.zero_grad()`\n",
    "\n",
    "4. **Tracking Metrics**\n",
    "   - Accumulate total loss across batches\n",
    "   - Count correctly predicted samples for accuracy\n",
    "   - Compute average loss and accuracy per epoch:\n",
    "     - `avg_loss = total_loss / len(train_loader.dataset)`\n",
    "     - `acc = correct / len(train_loader.dataset)`\n",
    "\n",
    "5. **Logging**\n",
    "   - Append epoch metrics to `train_losses` and `train_accuracies`\n",
    "   - Print progress in the format:  \n",
    "     ```\n",
    "     Epoch {epoch}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {acc:.4f}\n",
    "     ```\n",
    "\n",
    "#### Outputs\n",
    "- `train_losses`: List of average losses per epoch\n",
    "- `train_accuracies`: List of accuracies per epoch\n",
    "- Console logs for monitoring training progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2e2d75f-171d-4141-b561-40a94ee283b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_11608\\3015811450.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return frames_tensor, audio_tensor.unsqueeze(0), torch.tensor(label, dtype=torch.long)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_11608\\763122039.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.3077, Accuracy: 0.4655\n",
      "Epoch 2/30, Loss: 0.2720, Accuracy: 0.5506\n",
      "Epoch 3/30, Loss: 0.2366, Accuracy: 0.6228\n",
      "Epoch 4/30, Loss: 0.2272, Accuracy: 0.6453\n",
      "Epoch 5/30, Loss: 0.2186, Accuracy: 0.6533\n",
      "Epoch 6/30, Loss: 0.2292, Accuracy: 0.6276\n",
      "Epoch 7/30, Loss: 0.2200, Accuracy: 0.6469\n",
      "Epoch 8/30, Loss: 0.1993, Accuracy: 0.6934\n",
      "Epoch 9/30, Loss: 0.1887, Accuracy: 0.6854\n",
      "Epoch 10/30, Loss: 0.1736, Accuracy: 0.7464\n",
      "Epoch 11/30, Loss: 0.1723, Accuracy: 0.7432\n",
      "Epoch 12/30, Loss: 0.1700, Accuracy: 0.7303\n",
      "Epoch 13/30, Loss: 0.1611, Accuracy: 0.7657\n",
      "Epoch 14/30, Loss: 0.1489, Accuracy: 0.7833\n",
      "Epoch 15/30, Loss: 0.1419, Accuracy: 0.7865\n",
      "Epoch 16/30, Loss: 0.1429, Accuracy: 0.7994\n",
      "Epoch 17/30, Loss: 0.1422, Accuracy: 0.7978\n",
      "Epoch 18/30, Loss: 0.1386, Accuracy: 0.8042\n",
      "Epoch 19/30, Loss: 0.1276, Accuracy: 0.8170\n",
      "Epoch 20/30, Loss: 0.1245, Accuracy: 0.8122\n",
      "Epoch 21/30, Loss: 0.1353, Accuracy: 0.8234\n",
      "Epoch 22/30, Loss: 0.1148, Accuracy: 0.8347\n",
      "Epoch 23/30, Loss: 0.1150, Accuracy: 0.8587\n",
      "Epoch 24/30, Loss: 0.0979, Accuracy: 0.8700\n",
      "Epoch 25/30, Loss: 0.0979, Accuracy: 0.8668\n",
      "Epoch 26/30, Loss: 0.0965, Accuracy: 0.8732\n",
      "Epoch 27/30, Loss: 0.0984, Accuracy: 0.8684\n",
      "Epoch 28/30, Loss: 0.0892, Accuracy: 0.8876\n",
      "Epoch 29/30, Loss: 0.0869, Accuracy: 0.8925\n",
      "Epoch 30/30, Loss: 0.0846, Accuracy: 0.8828\n"
     ]
    }
   ],
   "source": [
    "accumulation_steps = 8\n",
    "num_epochs = 30\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, (img_seq, audio_spec, labels) in enumerate(train_loader):\n",
    "        img_seq = img_seq.to(device)\n",
    "        audio_spec = audio_spec.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(img_seq, audio_spec)\n",
    "            loss = criterion(outputs, labels) / accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1 == len(train_loader)):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = correct / len(train_loader.dataset)\n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c87dc72-18f8-45df-ad7c-332fcdb2af74",
   "metadata": {},
   "source": [
    "### This cell evaluates the model on the test set in evaluation mode (without gradient computation) and reports the overall test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6304145e-cc4b-44fe-87b4-0748443b9536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_12992\\3015811450.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return frames_tensor, audio_tensor.unsqueeze(0), torch.tensor(label, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8273\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for img_seq, audio_spec, labels in test_loader:\n",
    "        img_seq = img_seq.to(device)\n",
    "        audio_spec = audio_spec.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(img_seq, audio_spec)\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = correct / len(test_loader.dataset)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf11255d-ff6f-44e9-afd2-fa2623f7e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"cnn_lstm_model_224_32_audio_short.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
