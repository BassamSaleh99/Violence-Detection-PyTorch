{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86ddc4a1",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df963393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tempfile\n",
    "from moviepy import VideoFileClip\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import librosa\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb792a27",
   "metadata": {},
   "source": [
    "### Function: `block_difference`\n",
    "- Compares two frames by dividing them into blocks\n",
    "- Calculates the mean absolute difference between corresponding blocks \n",
    "- Returns the average block difference across the whole frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a03d719d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_difference(frame1, frame2, block_size=16):\n",
    "    height, width = frame1.shape\n",
    "    total_diff = 0\n",
    "    num_blocks = 0\n",
    "\n",
    "    for y in range(0, height, block_size):\n",
    "        for x in range(0, width, block_size):\n",
    "            block1 = frame1[y:y+block_size, x:x+block_size]\n",
    "            block2 = frame2[y:y+block_size, x:x+block_size]\n",
    "            if block1.shape == block2.shape:\n",
    "                diff = np.abs(block1.astype(int) - block2.astype(int)).mean()\n",
    "                total_diff += diff\n",
    "                num_blocks += 1\n",
    "\n",
    "    return total_diff / num_blocks if num_blocks > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27bedcd",
   "metadata": {},
   "source": [
    "### Function: `extract_keyframes_from_shot`\n",
    "- Extracts keyframes from a list of frames using block difference\n",
    "- Uses the first frame as reference, then adds new frames when difference exceeds a threshold\n",
    "- Resizes keyframes to 224x224\n",
    "- Ensures exactly 15 keyframes by padding with the last frame if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24e9dc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keyframes_from_shot(frames, fps, block_threshold=20):\n",
    "    duration = len(frames) / fps\n",
    "    target_num_keyframes = 15\n",
    "    keyframes = []\n",
    "\n",
    "    if len(frames) == 0:\n",
    "        return []\n",
    "\n",
    "    ref_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)\n",
    "    resized_frame = cv2.resize(frames[0], (224, 224))\n",
    "    keyframes.append(resized_frame)\n",
    "\n",
    "    for i, frame in enumerate(frames[1:], start=1):\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        diff = block_difference(ref_gray, gray)\n",
    "        if diff > block_threshold:\n",
    "            resized = cv2.resize(frame, (224, 224))\n",
    "            keyframes.append(resized)\n",
    "\n",
    "        if len(keyframes) >= target_num_keyframes:\n",
    "            break\n",
    "\n",
    "    while len(keyframes) < target_num_keyframes:\n",
    "        keyframes.append(keyframes[-1].copy())\n",
    "\n",
    "    return keyframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e08affc",
   "metadata": {},
   "source": [
    "### Function: `extract_audio_shot`\n",
    "- Extracts a segment of audio from a given audio file\n",
    "- Takes `start_time` and `end_time` in seconds\n",
    "- Saves the extracted audio segment as a `.wav` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56fea99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_shot(audio_path, start_time, end_time, save_path):\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "    start_ms = int(start_time * 1000)\n",
    "    end_ms = int(end_time * 1000)\n",
    "    segment = audio[start_ms:end_ms]\n",
    "    segment.export(save_path, format=\"wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a90adc9",
   "metadata": {},
   "source": [
    "### Function: `extract_audio_temp`\n",
    "- Extracts the entire audio track from a video\n",
    "- Saves it temporarily as a `.wav` file\n",
    "- Returns the path of the temporary audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62baec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_temp(video_path):\n",
    "    video = VideoFileClip(video_path)\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as temp_audio_file:\n",
    "        temp_audio_path = temp_audio_file.name\n",
    "    video.audio.write_audiofile(temp_audio_path)\n",
    "    return temp_audio_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5db51a5",
   "metadata": {},
   "source": [
    "### Function: `extract_shots_and_keyframes_with_audio`\n",
    "- Splits a video into shots based on histogram difference\n",
    "- Extracts **keyframes** from each shot using block difference\n",
    "- Saves the **corresponding audio segment** of each shot as `.wav`\n",
    "- Returns a list of samples, where each sample contains:  \n",
    "  - keyframes  \n",
    "  - audio file path  \n",
    "  - start time and end time of the shot  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11562f83",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0a6e797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_shots_and_keyframes_with_audio(video_path, hist_threshold=30, block_threshold=20, fps_cap=30, save_audio_dir=\"audio_shots\"):\n",
    "    os.makedirs(save_audio_dir, exist_ok=True)\n",
    "\n",
    "    audio_path = extract_audio_temp(video_path)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    fps = min(fps, fps_cap)\n",
    "\n",
    "    prev_hist = None\n",
    "    frames = []\n",
    "    all_samples = []\n",
    "    shot_start_frame = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n",
    "        hist = cv2.normalize(hist, hist).flatten()\n",
    "\n",
    "        if prev_hist is not None:\n",
    "            diff = cv2.compareHist(prev_hist, hist, cv2.HISTCMP_BHATTACHARYYA)\n",
    "            if diff > hist_threshold / 100:\n",
    "                shot_end_frame = shot_start_frame + len(frames)\n",
    "                start_time = shot_start_frame / fps\n",
    "                end_time = shot_end_frame / fps\n",
    "\n",
    "                keyframes = extract_keyframes_from_shot(frames, fps, block_threshold)\n",
    "\n",
    "                if keyframes:\n",
    "                    audio_filename = f\"{os.path.basename(video_path)}_{int(start_time*1000)}_{int(end_time*1000)}.wav\"\n",
    "                    audio_save_path = os.path.join(save_audio_dir, audio_filename)\n",
    "                    extract_audio_shot(audio_path, start_time, end_time, audio_save_path)\n",
    "\n",
    "                    all_samples.append({\n",
    "                        \"keyframes\": keyframes,\n",
    "                        \"audio_path\": audio_save_path,\n",
    "                        \"start_time\": start_time,\n",
    "                        \"end_time\": end_time\n",
    "                    })\n",
    "\n",
    "                shot_start_frame = shot_end_frame\n",
    "                frames = []\n",
    "\n",
    "        frames.append(frame.copy())\n",
    "        prev_hist = hist\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    os.remove(audio_path)\n",
    "\n",
    "    return all_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2c7fe9",
   "metadata": {},
   "source": [
    "### Function: `process_category_folder`\n",
    "Processes all video files in a category folder, extracts shots with keyframes and audio, and assigns the given label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c062407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_category_folder(category_path, audio_folder_path, label):\n",
    "    samples = []\n",
    "\n",
    "    for filename in os.listdir(category_path):\n",
    "        if filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "            video_path = os.path.join(category_path, filename)\n",
    "            audio_path = os.path.join(audio_folder_path, os.path.splitext(filename)[0] + \".wav\")\n",
    "            print(f\"Processing: {video_path}\")\n",
    "            shot_samples = extract_shots_and_keyframes_with_audio(video_path)\n",
    "            for sample in shot_samples:\n",
    "                sample[\"label\"] = label\n",
    "                samples.append(sample)\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95665406",
   "metadata": {},
   "source": [
    "### Main Script\n",
    "Iterates over all categories, processes their videos, and collects all samples into a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8973fa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"C:\\Users\\LEGION\\Desktop\\Violence Detection-PyTorch\"\n",
    "categories = [\"bloody\", \"explosions\", \"fight\", \"non-violence\"]\n",
    "\n",
    "all_samples = []\n",
    "\n",
    "for category in categories:\n",
    "    video_folder = os.path.join(base_path, category)\n",
    "    audio_folder = os.path.join(base_path, category)\n",
    "    category_samples = process_category_folder(video_folder, audio_folder, category)\n",
    "    all_samples.extend(category_samples)\n",
    "\n",
    "print(\"Total samples:\", len(all_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b51b78",
   "metadata": {},
   "source": [
    "### Data Preparation (Images + Audio + Labels)\n",
    "\n",
    "- Initializes empty lists for **image sequences**, **audio spectrograms**, and **labels**\n",
    "- Ensures each sample has exactly **15 frames** (by truncating or padding with the last frame)\n",
    "- Loads the corresponding **audio clip**, converts it into a **mel-spectrogram (128 mel bins)**, and pads/truncates it to a fixed length of **200 time steps**\n",
    "- Appends processed image frames, spectrogram, and label to their lists\n",
    "- Encodes labels into numeric form using `LabelEncoder`\n",
    "- Converts all lists into NumPy arrays and prints their shapes for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e04e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_images = []\n",
    "X_audio = []\n",
    "y_labels = []\n",
    "\n",
    "time_steps = 15\n",
    "mel_bins = 128\n",
    "max_audio_len = 200\n",
    "\n",
    "for sample in all_samples:\n",
    "    frames = sample[\"keyframes\"]\n",
    "    if len(frames) < time_steps:\n",
    "        frames += [frames[-1]] * (time_steps - len(frames))\n",
    "    else:\n",
    "        frames = frames[:time_steps]\n",
    "\n",
    "    y_audio, sr = librosa.load(sample[\"audio_path\"], sr=22050)\n",
    "    mel = librosa.feature.melspectrogram(y=y_audio, sr=sr, n_mels=mel_bins)\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "\n",
    "    if mel_db.shape[1] < max_audio_len:\n",
    "        pad_width = max_audio_len - mel_db.shape[1]\n",
    "        mel_db = np.pad(mel_db, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        mel_db = mel_db[:, :max_audio_len]\n",
    "\n",
    "    X_images.append(np.array(frames))\n",
    "    X_audio.append(mel_db)\n",
    "    y_labels.append(sample[\"label\"])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y_labels)\n",
    "\n",
    "X_images = np.array(X_images)\n",
    "X_audio = np.array(X_audio)\n",
    "y_encoded = np.array(y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fd8bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# joblib.dump(encoder, \"label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a262e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_img_train, X_img_test, X_audio_train, X_audio_test, y_train, y_test = train_test_split(\n",
    "    X_images, X_audio, y_encoded,\n",
    "    test_size=0.15,\n",
    "    stratify=y_encoded,\n",
    "    random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "905ff313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"train_data_with_audio_short.pkl\", \"wb\") as f:\n",
    "#     pickle.dump((X_img_train, X_audio_train, y_train), f)\n",
    "\n",
    "# with open(\"test_data_with_audio_short.pkl\", \"wb\") as f:\n",
    "#     pickle.dump((X_img_test, X_audio_test, y_test), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf1c384",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_data_with_audio_short.pkl\", \"rb\") as f:\n",
    "    X_img_train, X_audio_train, y_train = pickle.load(f)\n",
    "\n",
    "with open(\"test_data_with_audio_short.pkl\", \"rb\") as f:\n",
    "    X_img_test, X_audio_test, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426f4f13",
   "metadata": {},
   "source": [
    "### `CartoonViolenceMultiModalDataset` Class\n",
    "\n",
    "- Custom PyTorch `Dataset` for multimodal data (video frames + audio spectrograms + labels)\n",
    "- Supports **data augmentation** for image frames (random flips & rotations) when `augment=True`\n",
    "- `__len__`: returns the total number of samples\n",
    "- `__getitem__`:  \n",
    "  - Retrieves one sequence of frames, its spectrogram, and the label\n",
    "  - Transforms each frame into a tensor (with or without augmentation)\n",
    "  - Stacks all frames into a single tensor with shape `[T, C, H, W]` (time, channels, height, width)\n",
    "  - Converts spectrogram into a tensor of shape `[1, n_mels, time]`\n",
    "  - Returns `(frames_tensor, audio_tensor, label)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caed5fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartoonViolenceMultiModalDataset(Dataset):\n",
    "    def __init__(self, img_sequences, audio_spectrograms, labels, augment=False):\n",
    "        self.img_sequences = img_sequences\n",
    "        self.audio_spectrograms = audio_spectrograms\n",
    "        self.labels = labels\n",
    "        self.augment = augment\n",
    "\n",
    "        self.transform = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomRotation(15),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "        self.basic_transform = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_seq = self.img_sequences[idx]\n",
    "        audio_spec = self.audio_spectrograms[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        frames = []\n",
    "        for frame in img_seq:\n",
    "            frame_tensor = self.transform(frame) if self.augment else self.basic_transform(frame)\n",
    "            frames.append(frame_tensor)\n",
    "\n",
    "        frames_tensor = torch.stack(frames)\n",
    "        audio_tensor = torch.tensor(audio_spec, dtype=torch.float32)\n",
    "\n",
    "        return frames_tensor, audio_tensor.unsqueeze(0), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1984f148",
   "metadata": {},
   "source": [
    "### Class: `CNNLSTM_Audio`\n",
    "\n",
    "- **Purpose**: A multimodal neural network that combines **image sequences** and **audio spectrograms** for classification.  \n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Image Pathway\n",
    "- Uses a series of **CNN layers** to extract spatial features from each frame.  \n",
    "- Flattens the output and feeds it into an **LSTM** to capture temporal dependencies across frames.  \n",
    "- Produces a final image representation vector (size 128).  \n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Audio Pathway\n",
    "- Applies **1D CNN layers** to process the mel-spectrogram input (treating it like a sequence).  \n",
    "- Passes the output through an **LSTM** to capture temporal audio features.  \n",
    "- Produces a final audio representation vector (size 64).  \n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Fusion and Classification\n",
    "- Concatenates image and audio representations into a single feature vector `[128 + 64]`.  \n",
    "- Applies a **dropout layer** and a **fully connected layer** to classify into `num_classes`.  \n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Forward Pass\n",
    "1. Processes image sequence through CNN â†’ Flatten â†’ LSTM â†’ final frame output.  \n",
    "2. Processes audio spectrogram through CNN â†’ reshape â†’ LSTM â†’ final timestep output.  \n",
    "3. Concatenates image and audio outputs.  \n",
    "4. Passes fused vector through the classifier to get predictions.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b04ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTM_Audio(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNLSTM_Audio, self).__init__()\n",
    "\n",
    "        # Image pathway\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.lstm = nn.LSTM(128 * 28 * 28, 128, batch_first=True)\n",
    "\n",
    "        # Audio pathway\n",
    "        self.audio_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(64, 32, kernel_size=3, padding=1), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.audio_lstm = nn.LSTM(input_size=32, hidden_size=64, batch_first=True)\n",
    " \n",
    "        # Fusion and classifier\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128 + 64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_seq, audio_spec):\n",
    "        B, T, C, H, W = img_seq.shape\n",
    "\n",
    "        # Image branch\n",
    "        x = img_seq.view(-1, C, H, W)\n",
    "        x = self.cnn(x)\n",
    "        x = self.flatten(x)\n",
    "        x = x.view(B, T, -1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        # Audio branch\n",
    "        a = audio_spec.squeeze(1)  # [B, 128, time]\n",
    "        a = self.audio_conv(a)     # [B, 32, time']\n",
    "        a = a.permute(0, 2, 1)     # [B, time', 32]\n",
    "        a, _ = self.audio_lstm(a)\n",
    "        a = a[:, -1, :]\n",
    "\n",
    "        # Fusion\n",
    "        fused = torch.cat((x, a), dim=1)  # [B, 128 + 64]\n",
    "\n",
    "        # Classification\n",
    "        out = self.fc(fused)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b63b3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04ed6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CartoonViolenceMultiModalDataset(X_img_train, X_audio_train, y_train_tensor, augment=True)\n",
    "test_dataset = CartoonViolenceMultiModalDataset(X_img_test, X_audio_test, y_test_tensor, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba2216",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87f1cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNLSTM_Audio(num_classes=len(np.unique(y_train))).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23abbd40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
