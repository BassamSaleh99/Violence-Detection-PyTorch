{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c34bf87d-367a-43bc-bbe2-e652f512713d",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed60ca09-4bb4-4299-8731-c2307cd76ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tempfile\n",
    "from moviepy import VideoFileClip\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import librosa\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14103b9b-62c0-47f2-bfc6-0fc843c34410",
   "metadata": {},
   "source": [
    "### Function: `block_difference`\n",
    "- Compares two frames by dividing them into blocks\n",
    "- Calculates the mean absolute difference between corresponding blocks \n",
    "- Returns the average block difference across the whole frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91e60648-cce7-4612-94f6-61ededa3d1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_difference(frame1, frame2, block_size=16):\n",
    "    height, width = frame1.shape\n",
    "    total_diff = 0\n",
    "    num_blocks = 0\n",
    "\n",
    "    for y in range(0, height, block_size):\n",
    "        for x in range(0, width, block_size):\n",
    "            block1 = frame1[y:y+block_size, x:x+block_size]\n",
    "            block2 = frame2[y:y+block_size, x:x+block_size]\n",
    "            if block1.shape == block2.shape:\n",
    "                diff = np.abs(block1.astype(int) - block2.astype(int)).mean()\n",
    "                total_diff += diff\n",
    "                num_blocks += 1\n",
    "\n",
    "    return total_diff / num_blocks if num_blocks > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debf8ad3-0ff4-41a2-90a9-2f7b0e2e7f0a",
   "metadata": {},
   "source": [
    "### Function: `extract_keyframes_from_shot`\n",
    "- Extracts keyframes from a list of frames using block difference\n",
    "- Uses the first frame as reference, then adds new frames when difference exceeds a threshold\n",
    "- Resizes keyframes to 224x224\n",
    "- Ensures exactly 15 keyframes by padding with the last frame if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c8b9b1d-8d33-40da-a8c7-fa29c2f2197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keyframes_from_shot(frames, fps, block_threshold=20):\n",
    "    duration = len(frames) / fps\n",
    "    target_num_keyframes = 15\n",
    "    keyframes = []\n",
    "\n",
    "    if len(frames) == 0:\n",
    "        return []\n",
    "\n",
    "    ref_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)\n",
    "    resized_frame = cv2.resize(frames[0], (224, 224))\n",
    "    keyframes.append(resized_frame)\n",
    "\n",
    "    for i, frame in enumerate(frames[1:], start=1):\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        diff = block_difference(ref_gray, gray)\n",
    "        if diff > block_threshold:\n",
    "            resized = cv2.resize(frame, (224, 224))\n",
    "            keyframes.append(resized)\n",
    "\n",
    "        if len(keyframes) >= target_num_keyframes:\n",
    "            break\n",
    "\n",
    "    while len(keyframes) < target_num_keyframes:\n",
    "        keyframes.append(keyframes[-1].copy())\n",
    "\n",
    "    return keyframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c383752-5284-4745-9bec-0aeff31ddd0f",
   "metadata": {},
   "source": [
    "### Function: `extract_audio_shot`\n",
    "- Extracts a segment of audio from a given audio file\n",
    "- Takes `start_time` and `end_time` in seconds\n",
    "- Saves the extracted audio segment as a `.wav` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "716fa104-39c9-4b0d-9621-d15ac5515741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_shot(audio_path, start_time, end_time, save_path):\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "    start_ms = int(start_time * 1000)\n",
    "    end_ms = int(end_time * 1000)\n",
    "    segment = audio[start_ms:end_ms]\n",
    "    segment.export(save_path, format=\"wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8c047d-179f-4f39-8243-658c16c19cea",
   "metadata": {},
   "source": [
    "### Function: `extract_audio_temp`\n",
    "- Extracts the entire audio track from a video\n",
    "- Saves it temporarily as a `.wav` file\n",
    "- Returns the path of the temporary audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29a8fa3d-ec30-4bbc-b1ee-68ee3e3d9396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_temp(video_path):\n",
    "    video = VideoFileClip(video_path)\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as temp_audio_file:\n",
    "        temp_audio_path = temp_audio_file.name\n",
    "    video.audio.write_audiofile(temp_audio_path)\n",
    "    return temp_audio_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e24717-5574-46a6-86be-2fa9d5ee129d",
   "metadata": {},
   "source": [
    "### Function: `extract_shots_and_keyframes_with_audio`\n",
    "- Splits a video into shots based on histogram difference\n",
    "- Extracts **keyframes** from each shot using block difference\n",
    "- Saves the **corresponding audio segment** of each shot as `.wav`\n",
    "- Returns a list of samples, where each sample contains:  \n",
    "  - keyframes  \n",
    "  - audio file path  \n",
    "  - start time and end time of the shot  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "637628bd-94b6-4acf-86f4-10a55275a80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_shots_and_keyframes_with_audio(video_path, hist_threshold=30, block_threshold=20, fps_cap=30, save_audio_dir=\"audio_shots\"):\n",
    "    os.makedirs(save_audio_dir, exist_ok=True)\n",
    "\n",
    "    audio_path = extract_audio_temp(video_path)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    fps = min(fps, fps_cap)\n",
    "\n",
    "    prev_hist = None\n",
    "    frames = []\n",
    "    all_samples = []\n",
    "    shot_start_frame = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n",
    "        hist = cv2.normalize(hist, hist).flatten()\n",
    "\n",
    "        if prev_hist is not None:\n",
    "            diff = cv2.compareHist(prev_hist, hist, cv2.HISTCMP_BHATTACHARYYA)\n",
    "            if diff > hist_threshold / 100:\n",
    "                shot_end_frame = shot_start_frame + len(frames)\n",
    "                start_time = shot_start_frame / fps\n",
    "                end_time = shot_end_frame / fps\n",
    "\n",
    "                keyframes = extract_keyframes_from_shot(frames, fps, block_threshold)\n",
    "\n",
    "                if keyframes:\n",
    "                    audio_filename = f\"{os.path.basename(video_path)}_{int(start_time*1000)}_{int(end_time*1000)}.wav\"\n",
    "                    audio_save_path = os.path.join(save_audio_dir, audio_filename)\n",
    "                    extract_audio_shot(audio_path, start_time, end_time, audio_save_path)\n",
    "\n",
    "                    all_samples.append({\n",
    "                        \"keyframes\": keyframes,\n",
    "                        \"audio_path\": audio_save_path,\n",
    "                        \"start_time\": start_time,\n",
    "                        \"end_time\": end_time\n",
    "                    })\n",
    "\n",
    "                shot_start_frame = shot_end_frame\n",
    "                frames = []\n",
    "\n",
    "        frames.append(frame.copy())\n",
    "        prev_hist = hist\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    os.remove(audio_path)\n",
    "\n",
    "    return all_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3b08aa-5744-46e5-be62-e450ebb4c014",
   "metadata": {},
   "source": [
    "### Function: `process_category_folder`\n",
    "Processes all video files in a category folder, extracts shots with keyframes and audio, and assigns the given label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9d0e045-2c6f-4364-ab89-f3fd7e078630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_category_folder(category_path, audio_folder_path, label):\n",
    "    samples = []\n",
    "\n",
    "    for filename in os.listdir(category_path):\n",
    "        if filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "            video_path = os.path.join(category_path, filename)\n",
    "            audio_path = os.path.join(audio_folder_path, os.path.splitext(filename)[0] + \".wav\")\n",
    "            print(f\"Processing: {video_path}\")\n",
    "            shot_samples = extract_shots_and_keyframes_with_audio(video_path)\n",
    "            for sample in shot_samples:\n",
    "                sample[\"label\"] = label\n",
    "                samples.append(sample)\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7c9dbc-27d8-470e-b449-2b3b545cb94b",
   "metadata": {},
   "source": [
    "### Main Script\n",
    "Iterates over all categories, processes their videos, and collects all samples into a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4dc122-f02e-4f4a-8bd9-78412924d93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"C:\\Users\\LEGION\\Desktop\\Violence Detection-PyTorch\"\n",
    "categories = [\"bloody\", \"explosions\", \"fight\", \"non-violence\"]\n",
    "\n",
    "all_samples = []\n",
    "\n",
    "for category in categories:\n",
    "    video_folder = os.path.join(base_path, category)\n",
    "    audio_folder = os.path.join(base_path, category)\n",
    "    category_samples = process_category_folder(video_folder, audio_folder, category)\n",
    "    all_samples.extend(category_samples)\n",
    "\n",
    "print(\"Total samples:\", len(all_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e36f1a5-2903-490c-b679-99012bf2c6de",
   "metadata": {},
   "source": [
    "### Data Preparation (Images + Audio + Labels)\n",
    "\n",
    "- Initializes empty lists for **image sequences**, **audio spectrograms**, and **labels**\n",
    "- Ensures each sample has exactly **15 frames** (by truncating or padding with the last frame)\n",
    "- Loads the corresponding **audio clip**, converts it into a **mel-spectrogram (128 mel bins)**, and pads/truncates it to a fixed length of **200 time steps**\n",
    "- Appends processed image frames, spectrogram, and label to their lists\n",
    "- Encodes labels into numeric form using `LabelEncoder`\n",
    "- Converts all lists into NumPy arrays and prints their shapes for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec91479c-8f52-4076-91df-4fefb0109e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_images = []\n",
    "X_audio = []\n",
    "y_labels = []\n",
    "\n",
    "time_steps = 15\n",
    "mel_bins = 128\n",
    "max_audio_len = 200\n",
    "\n",
    "for sample in all_samples:\n",
    "    frames = sample[\"keyframes\"]\n",
    "    if len(frames) < time_steps:\n",
    "        frames += [frames[-1]] * (time_steps - len(frames))\n",
    "    else:\n",
    "        frames = frames[:time_steps]\n",
    "\n",
    "    y_audio, sr = librosa.load(sample[\"audio_path\"], sr=22050)\n",
    "    mel = librosa.feature.melspectrogram(y=y_audio, sr=sr, n_mels=mel_bins)\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "\n",
    "    if mel_db.shape[1] < max_audio_len:\n",
    "        pad_width = max_audio_len - mel_db.shape[1]\n",
    "        mel_db = np.pad(mel_db, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        mel_db = mel_db[:, :max_audio_len]\n",
    "\n",
    "    X_images.append(np.array(frames))\n",
    "    X_audio.append(mel_db)\n",
    "    y_labels.append(sample[\"label\"])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y_labels)\n",
    "\n",
    "X_images = np.array(X_images)\n",
    "X_audio = np.array(X_audio)\n",
    "y_encoded = np.array(y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb0663c2-7f44-4051-a08b-07089db9b548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_encoder.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import joblib\n",
    "# joblib.dump(encoder, \"label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d02e56df-cd93-4e08-a6b3-7c2eec54b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_img_train, X_img_test, X_audio_train, X_audio_test, y_train, y_test = train_test_split(\n",
    "    X_images, X_audio, y_encoded,\n",
    "    test_size=0.15,\n",
    "    stratify=y_encoded,\n",
    "    random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a5a1a9f-94ee-4543-82dc-e23c1725ffce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"train_data_with_audio_short.pkl\", \"wb\") as f:\n",
    "#     pickle.dump((X_img_train, X_audio_train, y_train), f)\n",
    "\n",
    "# with open(\"test_data_with_audio_short.pkl\", \"wb\") as f:\n",
    "#     pickle.dump((X_img_test, X_audio_test, y_test), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2b81c82-8058-4604-adc1-fc8396f1b08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_data_with_audio_short.pkl\", \"rb\") as f:\n",
    "    X_img_train, X_audio_train, y_train = pickle.load(f)\n",
    "\n",
    "with open(\"test_data_with_audio_short.pkl\", \"rb\") as f:\n",
    "    X_img_test, X_audio_test, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cdb69b-bfec-4a55-bae3-40c110b93fdf",
   "metadata": {},
   "source": [
    "### `CartoonViolenceMultiModalDataset` Class\n",
    "\n",
    "- Custom PyTorch `Dataset` for multimodal data (video frames + audio spectrograms + labels)\n",
    "- Supports **data augmentation** for image frames (random flips & rotations) when `augment=True`\n",
    "- `__len__`: returns the total number of samples\n",
    "- `__getitem__`:  \n",
    "  - Retrieves one sequence of frames, its spectrogram, and the label\n",
    "  - Transforms each frame into a tensor (with or without augmentation)\n",
    "  - Stacks all frames into a single tensor with shape `[T, C, H, W]` (time, channels, height, width)\n",
    "  - Converts spectrogram into a tensor of shape `[1, n_mels, time]`\n",
    "  - Returns `(frames_tensor, audio_tensor, label)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f495206-195a-4d53-becb-a8147a2c18ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartoonViolenceMultiModalDataset(Dataset):\n",
    "    def __init__(self, img_sequences, audio_spectrograms, labels, augment=False):\n",
    "        self.img_sequences = img_sequences\n",
    "        self.audio_spectrograms = audio_spectrograms\n",
    "        self.labels = labels\n",
    "        self.augment = augment\n",
    "\n",
    "        self.transform = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomRotation(15),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "        self.basic_transform = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_seq = self.img_sequences[idx]\n",
    "        audio_spec = self.audio_spectrograms[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        frames = []\n",
    "        for frame in img_seq:\n",
    "            frame_tensor = self.transform(frame) if self.augment else self.basic_transform(frame)\n",
    "            frames.append(frame_tensor)\n",
    "\n",
    "        frames_tensor = torch.stack(frames)\n",
    "        audio_tensor = torch.tensor(audio_spec, dtype=torch.float32)\n",
    "\n",
    "        return frames_tensor, audio_tensor.unsqueeze(0), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5048790-229b-4d7f-92fe-e3f54decf214",
   "metadata": {},
   "source": [
    "### Class: `CNNLSTM_Audio`\n",
    "\n",
    "- **Purpose**: A multimodal neural network that combines **image sequences** and **audio spectrograms** for classification.  \n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Image Pathway\n",
    "- Uses a series of **CNN layers** to extract spatial features from each frame.  \n",
    "- Flattens the output and feeds it into an **LSTM** to capture temporal dependencies across frames.  \n",
    "- Produces a final image representation vector (size 128).  \n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Audio Pathway\n",
    "- Applies **1D CNN layers** to process the mel-spectrogram input (treating it like a sequence).  \n",
    "- Passes the output through an **LSTM** to capture temporal audio features.  \n",
    "- Produces a final audio representation vector (size 64).  \n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Fusion and Classification\n",
    "- Concatenates image and audio representations into a single feature vector `[128 + 64]`.  \n",
    "- Applies a **dropout layer** and a **fully connected layer** to classify into `num_classes`.  \n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Forward Pass\n",
    "1. Processes image sequence through CNN â†’ Flatten â†’ LSTM â†’ final frame output.  \n",
    "2. Processes audio spectrogram through CNN â†’ reshape â†’ LSTM â†’ final timestep output.  \n",
    "3. Concatenates image and audio outputs.  \n",
    "4. Passes fused vector through the classifier to get predictions.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fc786db-9cb4-41c2-8d23-5f640c96c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTM_Audio(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNLSTM_Audio, self).__init__()\n",
    "\n",
    "        # Image pathway\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.lstm = nn.LSTM(128 * 28 * 28, 128, batch_first=True)\n",
    "\n",
    "        # Audio pathway\n",
    "        self.audio_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(64, 32, kernel_size=3, padding=1), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.audio_lstm = nn.LSTM(input_size=32, hidden_size=64, batch_first=True)\n",
    " \n",
    "        # Fusion and classifier\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128 + 64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_seq, audio_spec):\n",
    "        B, T, C, H, W = img_seq.shape\n",
    "\n",
    "        # Image branch\n",
    "        x = img_seq.view(-1, C, H, W)\n",
    "        x = self.cnn(x)\n",
    "        x = self.flatten(x)\n",
    "        x = x.view(B, T, -1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        # Audio branch\n",
    "        a = audio_spec.squeeze(1)  # [B, 128, time]\n",
    "        a = self.audio_conv(a)     # [B, 32, time']\n",
    "        a = a.permute(0, 2, 1)     # [B, time', 32]\n",
    "        a, _ = self.audio_lstm(a)\n",
    "        a = a[:, -1, :]\n",
    "\n",
    "        # Fusion\n",
    "        fused = torch.cat((x, a), dim=1)  # [B, 128 + 64]\n",
    "\n",
    "        # Classification\n",
    "        out = self.fc(fused)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b321ec56-fd40-4446-bd1d-9f5b935c2636",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5896538-0031-4790-a83b-bc147e1651cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CartoonViolenceMultiModalDataset(X_img_train, X_audio_train, y_train_tensor, augment=True)\n",
    "test_dataset = CartoonViolenceMultiModalDataset(X_img_test, X_audio_test, y_test_tensor, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b09dd6d4-9090-4018-bf0e-0aea4cc01ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3163a508-f781-40ac-a4bb-648730214ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNLSTM_Audio(num_classes=len(np.unique(y_train))).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdff1f4-6aa8-463d-a484-ed17bbd0a8c4",
   "metadata": {},
   "source": [
    "### Defines the loss function (CrossEntropy), optimizer (Adam), and gradient scaler for mixed-precision training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90cc0af6-7fbf-435c-b230-a029acfdd09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_11608\\1423562292.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05db098f-b35a-4b80-846c-f86df9089101",
   "metadata": {},
   "source": [
    "### Gradient Accumulation Training Loop\n",
    "\n",
    "This training loop uses **gradient accumulation** to effectively simulate a larger batch size by accumulating gradients over several mini-batches before performing an optimizer step. Mixed precision training with `torch.cuda.amp` is also used for efficiency\n",
    "\n",
    "#### Key Hyperparameters\n",
    "- `accumulation_steps = 8`: Number of steps to accumulate gradients before updating weights\n",
    "- `num_epochs = 30`: Total number of training epochs\n",
    "\n",
    "#### Workflow\n",
    "1. **Initialization**\n",
    "   - Track training losses and accuracies in lists\n",
    "   - Set the model to training mode with `model.train()`\n",
    "   - Initialize optimizer gradients with `optimizer.zero_grad()`\n",
    "\n",
    "2. **Mini-batch Training**\n",
    "   - For each batch `(img_seq, audio_spec, labels)`:\n",
    "     - Move inputs and labels to the device (GPU/CPU)\n",
    "     - Perform forward pass under `autocast()` for mixed precision\n",
    "     - Compute loss, normalized by `accumulation_steps`\n",
    "     - Backpropagate using `scaler.scale(loss).backward()`\n",
    "\n",
    "3. **Gradient Accumulation**\n",
    "   - After `accumulation_steps` iterations (or at the last batch):\n",
    "     - Update model weights with `scaler.step(optimizer)`\n",
    "     - Update the scaler with `scaler.update()`.\n",
    "     - Reset gradients with `optimizer.zero_grad()`\n",
    "\n",
    "4. **Tracking Metrics**\n",
    "   - Accumulate total loss across batches\n",
    "   - Count correctly predicted samples for accuracy\n",
    "   - Compute average loss and accuracy per epoch:\n",
    "     - `avg_loss = total_loss / len(train_loader.dataset)`\n",
    "     - `acc = correct / len(train_loader.dataset)`\n",
    "\n",
    "5. **Logging**\n",
    "   - Append epoch metrics to `train_losses` and `train_accuracies`\n",
    "   - Print progress in the format:  \n",
    "     ```\n",
    "     Epoch {epoch}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {acc:.4f}\n",
    "     ```\n",
    "\n",
    "#### Outputs\n",
    "- `train_losses`: List of average losses per epoch\n",
    "- `train_accuracies`: List of accuracies per epoch\n",
    "- Console logs for monitoring training progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2e2d75f-171d-4141-b561-40a94ee283b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_11608\\3015811450.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return frames_tensor, audio_tensor.unsqueeze(0), torch.tensor(label, dtype=torch.long)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_11608\\763122039.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.3077, Accuracy: 0.4655\n",
      "Epoch 2/30, Loss: 0.2720, Accuracy: 0.5506\n",
      "Epoch 3/30, Loss: 0.2366, Accuracy: 0.6228\n",
      "Epoch 4/30, Loss: 0.2272, Accuracy: 0.6453\n",
      "Epoch 5/30, Loss: 0.2186, Accuracy: 0.6533\n",
      "Epoch 6/30, Loss: 0.2292, Accuracy: 0.6276\n",
      "Epoch 7/30, Loss: 0.2200, Accuracy: 0.6469\n",
      "Epoch 8/30, Loss: 0.1993, Accuracy: 0.6934\n",
      "Epoch 9/30, Loss: 0.1887, Accuracy: 0.6854\n",
      "Epoch 10/30, Loss: 0.1736, Accuracy: 0.7464\n",
      "Epoch 11/30, Loss: 0.1723, Accuracy: 0.7432\n",
      "Epoch 12/30, Loss: 0.1700, Accuracy: 0.7303\n",
      "Epoch 13/30, Loss: 0.1611, Accuracy: 0.7657\n",
      "Epoch 14/30, Loss: 0.1489, Accuracy: 0.7833\n",
      "Epoch 15/30, Loss: 0.1419, Accuracy: 0.7865\n",
      "Epoch 16/30, Loss: 0.1429, Accuracy: 0.7994\n",
      "Epoch 17/30, Loss: 0.1422, Accuracy: 0.7978\n",
      "Epoch 18/30, Loss: 0.1386, Accuracy: 0.8042\n",
      "Epoch 19/30, Loss: 0.1276, Accuracy: 0.8170\n",
      "Epoch 20/30, Loss: 0.1245, Accuracy: 0.8122\n",
      "Epoch 21/30, Loss: 0.1353, Accuracy: 0.8234\n",
      "Epoch 22/30, Loss: 0.1148, Accuracy: 0.8347\n",
      "Epoch 23/30, Loss: 0.1150, Accuracy: 0.8587\n",
      "Epoch 24/30, Loss: 0.0979, Accuracy: 0.8700\n",
      "Epoch 25/30, Loss: 0.0979, Accuracy: 0.8668\n",
      "Epoch 26/30, Loss: 0.0965, Accuracy: 0.8732\n",
      "Epoch 27/30, Loss: 0.0984, Accuracy: 0.8684\n",
      "Epoch 28/30, Loss: 0.0892, Accuracy: 0.8876\n",
      "Epoch 29/30, Loss: 0.0869, Accuracy: 0.8925\n",
      "Epoch 30/30, Loss: 0.0846, Accuracy: 0.8828\n"
     ]
    }
   ],
   "source": [
    "accumulation_steps = 8\n",
    "num_epochs = 30\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, (img_seq, audio_spec, labels) in enumerate(train_loader):\n",
    "        img_seq = img_seq.to(device)\n",
    "        audio_spec = audio_spec.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(img_seq, audio_spec)\n",
    "            loss = criterion(outputs, labels) / accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1 == len(train_loader)):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = correct / len(train_loader.dataset)\n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c87dc72-18f8-45df-ad7c-332fcdb2af74",
   "metadata": {},
   "source": [
    "### This cell evaluates the model on the test set in evaluation mode (without gradient computation) and reports the overall test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6304145e-cc4b-44fe-87b4-0748443b9536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_12992\\3015811450.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return frames_tensor, audio_tensor.unsqueeze(0), torch.tensor(label, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8273\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for img_seq, audio_spec, labels in test_loader:\n",
    "        img_seq = img_seq.to(device)\n",
    "        audio_spec = audio_spec.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(img_seq, audio_spec)\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = correct / len(test_loader.dataset)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf11255d-ff6f-44e9-afd2-fa2623f7e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"cnn_lstm_model_224_32_audio_short.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
